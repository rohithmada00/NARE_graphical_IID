{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "zs9xYzKTzuD4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_positive_definite(M, epsilon=1e-3, min_threshold=1e-3):\n",
        "    \"\"\"Ensure M is well-conditioned ; add small value to diagonal if needed\"\"\"\n",
        "    min_eig = np.min(np.linalg.eigvals(M))\n",
        "    \n",
        "    if min_eig < min_threshold:\n",
        "        print(f\"Minimum eigenvalue too small ({min_eig:.2e}), adding {epsilon} to diagonal elements.\")\n",
        "        M += np.eye(M.shape[0]) * (abs(min_eig) + epsilon)\n",
        "    \n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "je8SzuFHlOOR"
      },
      "outputs": [],
      "source": [
        "def make_sparse_spd_matrix(\n",
        "    n_dim=10,\n",
        "    alpha=0.95,\n",
        "    norm_diag=True,\n",
        "    smallest_coef=0.1,\n",
        "    largest_coef=0.9,\n",
        "    random_state=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a sparse symmetric positive definite (SPD) matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_dim : int, default=10\n",
        "        The size of the random matrix to generate.\n",
        "\n",
        "    alpha : float, default=0.95\n",
        "        The probability that a coefficient is zero, controlling sparsity.\n",
        "        Higher values mean more sparsity. Should be between 0 and 1.\n",
        "\n",
        "    norm_diag : bool, default=False\n",
        "        If True, normalizes the matrix so that the diagonal elements are all 1.\n",
        "\n",
        "    smallest_coef : float, default=0.1\n",
        "        The smallest coefficient in the randomly generated values (between 0 and 1).\n",
        "\n",
        "    largest_coef : float, default=0.9\n",
        "        The largest coefficient in the randomly generated values (between 0 and 1).\n",
        "\n",
        "    random_state : int or None, default=None\n",
        "        Seed for random number generation, ensuring reproducible results.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray or sparse matrix\n",
        "        The generated sparse SPD matrix as a dense ndarray.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Start with a negative identity matrix, which will form the basis of the Cholesky factor.\n",
        "    chol = -sp.eye(n_dim, format=\"csc\")\n",
        "\n",
        "    # Generate a random sparse lower triangular matrix to add sparsity\n",
        "    aux = sp.random(\n",
        "        m=n_dim, n=n_dim, density=1 - alpha,\n",
        "        data_rvs=lambda x: rng.uniform(low=smallest_coef, high=largest_coef, size=x),\n",
        "        format=\"csc\"\n",
        "    )\n",
        "    aux = sp.tril(aux, k=-1, format=\"csc\")\n",
        "\n",
        "    # Randomly permute rows and columns to avoid asymmetries\n",
        "    permutation = rng.permutation(n_dim)\n",
        "    aux = aux[permutation].T[permutation]\n",
        "\n",
        "    # Add the sparse auxiliary matrix to the Cholesky factor\n",
        "    chol += aux\n",
        "\n",
        "    # Form the SPD matrix by taking the product of the Cholesky factor with its transpose\n",
        "    prec = chol.T @ chol\n",
        "\n",
        "    # Optionally normalize the diagonal to 1\n",
        "    if norm_diag:\n",
        "        d = sp.diags(1.0 / np.sqrt(prec.diagonal()))\n",
        "        prec = d @ prec @ d\n",
        "    prec = ensure_positive_definite(prec.toarray())\n",
        "\n",
        "    return prec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "c49D6hHYld0j"
      },
      "outputs": [],
      "source": [
        "import scipy.linalg as la\n",
        "from scipy.linalg import solve_sylvester, norm\n",
        "import networkx as nx\n",
        "\n",
        "# Fix random number generator for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "8tgDmkf5Lg2o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.linalg as la\n",
        "\n",
        "def generate_sparse_covariance(n, sample_scaling=1.0, alpha=0.8, random_state=42):\n",
        "    \"\"\"\n",
        "    Generate a sparse inverse covariance matrix B, compute its associated covariance matrix E,\n",
        "    and generate samples from a multivariate normal distribution with covariance E.\n",
        "\n",
        "    Parameters:\n",
        "    - n (int): Dimension of the matrix.\n",
        "    - sample_scaling (float): Scaling factor for the number of samples (N = sample_scaling * d^2 log2(n)).\n",
        "    - alpha (float): Sparsity level for the sparse SPD matrix.\n",
        "    - random_state (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - B (np.ndarray): Sparse inverse covariance matrix (precision matrix).\n",
        "    - y_samples (np.ndarray): Generated samples following N(0, E).\n",
        "    - S (np.ndarray): Sample covariance matrix from the generated samples.\n",
        "    - N (int): Computed number of samples.\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Create sparse PSD matrix (B)\n",
        "    B = make_sparse_spd_matrix(n_dim=n, alpha=alpha, norm_diag=True, random_state=random_state)\n",
        "\n",
        "    # Compute degree d as the maximum number of nonzero entries per row in B (excluding diagonal)\n",
        "    d = np.max(np.sum(B != 0, axis=1)) - 1  # Exclude diagonal elements\n",
        "    print(\"Max degree in B\", d)\n",
        "\n",
        "    # Compute the required number of samples with log \n",
        "    N = int(sample_scaling * (d**2 * np.log(n)))\n",
        "    N = max(N, n)  # Ensure N is at least n for stability\n",
        "\n",
        "    # Compute true inverse covariance matrix (Strue)\n",
        "    Strue = np.linalg.matrix_power(B, 2)\n",
        "\n",
        "    # Compute covariance matrix (E)\n",
        "    E = np.linalg.inv(Strue)\n",
        "\n",
        "    # Generate N samples Y ~ N(0, E)\n",
        "    y_samples = la.sqrtm(E).dot(np.random.randn(n, N))\n",
        "\n",
        "    # Calculate sample covariance matrix\n",
        "    S = np.cov(y_samples)\n",
        "\n",
        "    return B, S, N\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "tQGJnPihlgjt"
      },
      "outputs": [],
      "source": [
        "def newton_nare(A, B, C, D, X0, tol=1e-13, kmax=30):\n",
        "    \"\"\"\n",
        "    Newton's method for solving the Nonlinear Algebraic Riccati Equation (NARE):\n",
        "    C + XA + DX - XBX = 0\n",
        "    \"\"\"\n",
        "    X = X0.copy()\n",
        "    k = 0\n",
        "    err = 1\n",
        "\n",
        "    while err > tol and k < kmax:\n",
        "        # Compute residual RX = C + XA + DX - XBX\n",
        "        RX = C + X @ A + D @ X - X @ B @ X\n",
        "\n",
        "        # Solve the Sylvester equation (D - XB)H + H(A - BX) = -RX for H\n",
        "        H = solve_sylvester(D - X @ B, A - B @ X, -RX)\n",
        "\n",
        "        # Update X\n",
        "        X = X + H\n",
        "\n",
        "        # Calculate the error\n",
        "        err = norm(H, 'fro') / (1 + norm(X, 'fro'))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # Check if the solution converged\n",
        "    if k == kmax:\n",
        "        print(\"Warning: reached the maximum number of iterations without convergence.\")\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "sdGMJuRwnPup"
      },
      "outputs": [],
      "source": [
        "# Soft thresholding function\n",
        "def soft_thresholding(x, threshold):\n",
        "    \"\"\"Applies soft-thresholding elementwise.\"\"\"\n",
        "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "h2vCOGBFnT5J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ADMM Algorithm for Elastic-Net Penalized Precision Matrix Estimation\n",
        "def admm_precision_matrix(S, lambda_, rho=1.0, max_iter=100, tol=1e-4):\n",
        "    \"\"\"\n",
        "    ADMM algorithm for precision matrix estimation with elastic-net penalty.\n",
        "    \"\"\"\n",
        "    n = S.shape[0]\n",
        "    Z = np.zeros((n, n))\n",
        "    Lambda = np.zeros((n, n))\n",
        "    I = np.eye(n)  # Identity matrix\n",
        "\n",
        "    # Initial B (can be initialized as identity matrix)\n",
        "    B = np.eye(n)\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        # Step 1: Update B using Newton NARE\n",
        "        # Here, we set up the matrices to solve the NARE: A3 + XA1 + A4X - XA2X = 0\n",
        "        A3 = - 2 * I\n",
        "        A4 = Lambda - rho * Z\n",
        "        A1 = 0 * I\n",
        "        A2 = - (2 * S + rho * I)\n",
        "        X0 = B  # Initial guess for Newton NARE\n",
        "\n",
        "        # Solve for the new B using Newton NARE\n",
        "        B_new = newton_nare(A1, A2, A3, A4, X0)\n",
        "\n",
        "        # Step 2: Update Z elementwise using soft-thresholding\n",
        "        Z_new = soft_thresholding(rho * B_new + Lambda, lambda_)\n",
        "        Z_new = Z_new / rho\n",
        "\n",
        "        # Step 3: Update Lambda (Lagrange multiplier)\n",
        "        Lambda_new = Lambda + rho * (B_new - Z_new)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm(B_new - B, ord='fro') < tol:\n",
        "            print(f\"ADMM Converged after {k+1} iterations.\")\n",
        "            break\n",
        "        elif k == max_iter-1 :\n",
        "            print(f\"ADMM failed to converge after {k+1} iterations.\")\n",
        "\n",
        "        # Update for the next iteration\n",
        "        B, Z, Lambda = B_new, Z_new, Lambda_new      \n",
        "\n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "U0IvvFLPs1u-"
      },
      "outputs": [],
      "source": [
        "#Thresholding B_estimate\n",
        "def hard_threshold(B_estimate,threshold):\n",
        "  return np.where(np.abs(B_estimate) > threshold, B_estimate, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_metrics_vs_lambda(lambda_values, n = 10, sample_scaling = 1.0, alpha=1, rho=1.0, max_iter=100, tol=1e-4, threshold=1e-4, log_dir=\"experiment_logs\"):    \n",
        "    \"\"\"\n",
        "    Evaluate Frobenius norm, KL divergence, MCC, Sensitivity, and Specificity\n",
        "    of the estimated precision matrix at various lambda values.\n",
        "    \"\"\"\n",
        "\n",
        "    B, S, N = generate_sparse_covariance(n, sample_scaling= sample_scaling, alpha= alpha)\n",
        "    ground_truth_adjacency = (B != 0).astype(int).flatten()\n",
        "    logdet_B = np.log(np.linalg.det(B))\n",
        "\n",
        "    print(\"Number of samples:\", N)\n",
        "    print(\"Minimum eigenvalue:\", np.min(np.linalg.eigvals(B)))\n",
        "\n",
        "    metrics = {\n",
        "        \"lambda\": [],\n",
        "        \"Fnorm\": [],\n",
        "        \"KL\": [],\n",
        "        \"MCC\": [],\n",
        "        \"sensitivity\": [],\n",
        "        \"specificity\": [],\n",
        "        \"f1\": [], \n",
        "        \"plot_paths\": []\n",
        "    }\n",
        "\n",
        "\n",
        "    for lambda_ in lambda_values:\n",
        "        # Compute estimated precision matrix\n",
        "        B_est  = admm_precision_matrix(S, lambda_, rho, max_iter, tol)\n",
        "        \n",
        "        # Apply thresholding\n",
        "        B_est_thresholded  = hard_threshold(B_est, threshold)\n",
        "\n",
        "        # Plot sparsity patterns and save them\n",
        "        plt.figure(figsize=(6, 12))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.spy(B)\n",
        "        plt.title('B matrix', fontsize=16)\n",
        "\n",
        "        # Placeholder for estimated matrix\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.spy(B_est_thresholded)\n",
        "        plt.title('B_hat (Estimated)', fontsize=16)\n",
        "\n",
        "        # plt.subplot(2, 2, 3)\n",
        "        # plt.spy(E)\n",
        "        # plt.title('True covariance', fontsize=16)\n",
        "\n",
        "        # plt.subplot(2, 2, 4)\n",
        "        # plt.spy(S)\n",
        "        # plt.title('Sample covariance', fontsize=16)\n",
        "\n",
        "        plot_path = os.path.join(log_dir, f\"sparsity_patterns_lambda_{lambda_:.3f}.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        plt.close()\n",
        "        \n",
        "        metrics[\"plot_paths\"].append(plot_path)\n",
        "\n",
        "        # Fnorm_val\n",
        "        Fnorm_val = np.linalg.norm(B - B_est, ord='fro')\n",
        "\n",
        "        # KL divergence\n",
        "        try:\n",
        "            inv_Best = np.linalg.inv(B_est)\n",
        "            logdet_Best = np.log(np.linalg.det(B_est))\n",
        "            trace_term = np.trace(inv_Best @ B)\n",
        "            KL_val = -logdet_Best + trace_term + logdet_B - n\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If B_est is singular, define KL as +∞ or some large number\n",
        "            KL_val = np.inf\n",
        "        \n",
        "        predicted_adjacency = (B_est_thresholded != 0).astype(int).flatten()\n",
        "        cm = confusion_matrix(ground_truth_adjacency, predicted_adjacency, labels=[0,1])\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "        # MCC = (TP*TN - FP*FN) / sqrt( (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN) )\n",
        "        # We'll do a safe-check:\n",
        "        denom = (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)\n",
        "        if denom == 0:\n",
        "            MCC_val = 0.0\n",
        "        else:\n",
        "            MCC_val = ((TP*TN) - (FP*FN)) / np.sqrt(denom)\n",
        "        \n",
        "        # Sensitivity = TP / (TP + FN)  [recall]\n",
        "        sensitivity_val = TP / (TP + FN) if (TP+FN) > 0 else 0.0\n",
        "        # Specificity = TN / (TN + FP)\n",
        "        specificity_val = TN / (TN + FP) if (TN+FP) > 0 else 0.0\n",
        "        # F1 score\n",
        "        f1_val = f1_score(ground_truth_adjacency, predicted_adjacency)\n",
        "\n",
        "        metrics[\"lambda\"].append(lambda_)\n",
        "        metrics[\"Fnorm\"].append(Fnorm_val)\n",
        "        metrics[\"KL\"].append(KL_val)\n",
        "        metrics[\"MCC\"].append(MCC_val)\n",
        "        metrics[\"sensitivity\"].append(sensitivity_val)\n",
        "        metrics[\"specificity\"].append(specificity_val)\n",
        "        metrics[\"f1\"].append(f1_val)\n",
        "        # metrics[\"plot_paths\"].append(plot_path)\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V2nbZQbUthdT",
        "outputId": "b5159b66-0f83-4b92-92f9-3cf7caa3c334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating metrics for {'lambda_values': [0.0, 0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.38888888888888884, 0.4444444444444444, 0.5], 'n': 200, 'sample_scaling': 5, 'alpha': 0.98, 'rho': 8.0, 'max_iter': 500, 'tol': 0.0001, 'threshold': 0.01, 'log_dir': 'experiment_logs/n_200_scale_5/alpha_0.980'}\n",
            "Max degree in B 28\n",
            "Number of samples: 20769\n",
            "Minimum eigenvalue: 0.021734224482170314\n",
            "ADMM Converged after 149 iterations.\n",
            "ADMM Converged after 94 iterations.\n",
            "ADMM Converged after 111 iterations.\n",
            "ADMM Converged after 116 iterations.\n",
            "ADMM Converged after 143 iterations.\n",
            "ADMM Converged after 160 iterations.\n",
            "ADMM Converged after 176 iterations.\n",
            "ADMM Converged after 185 iterations.\n",
            "ADMM Converged after 197 iterations.\n",
            "ADMM Converged after 204 iterations.\n",
            "Best lambda by F1 = 0.056\n"
          ]
        }
      ],
      "source": [
        "# Updated main loop\n",
        "n = 200\n",
        "sample_scaling = 5\n",
        "\n",
        "for _alpha in [0.98]:\n",
        "    log_dir = f\"experiment_logs/n_{n}_scale_{sample_scaling}/alpha_{_alpha:.3f}\"\n",
        "    # Create a directory to save logs if it doesn’t exist\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # params\n",
        "    args = {\n",
        "        \"lambda_values\": np.linspace(0, 0.5, 10).tolist(),\n",
        "        \"n\": n,\n",
        "        \"sample_scaling\": sample_scaling,\n",
        "        \"alpha\": _alpha,\n",
        "        \"rho\": 8.0,\n",
        "        \"max_iter\": 500,\n",
        "        \"tol\": 1e-4,\n",
        "        \"threshold\": 1e-2,\n",
        "        \"log_dir\": log_dir\n",
        "    }\n",
        "    print(f\"Evaluating metrics for {args}\")\n",
        "\n",
        "    # Save args to a JSON file (for reproducibility)\n",
        "    json_path = os.path.join(log_dir, \"params.json\")\n",
        "    with open(json_path, \"w\") as json_file:\n",
        "        json.dump(args, json_file, indent=4)\n",
        "\n",
        "    # Call our updated evaluate_metrics_vs_lambda\n",
        "    metrics = evaluate_metrics_vs_lambda(**args)\n",
        "\n",
        "    # Convert metrics to a DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    # Save metrics to CSV or Excel\n",
        "    csv_path = os.path.join(log_dir, \"metrics.csv\")\n",
        "    metrics_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Pick best lambda by F1\n",
        "    best_idx = metrics_df[\"f1\"].idxmax()\n",
        "    best_lambda = metrics_df.loc[best_idx, \"lambda\"]\n",
        "    print(f\"Best lambda by F1 = {best_lambda:.3f}\")\n",
        "\n",
        "    # print the corresponding row of metric values\n",
        "    # best_row = metrics_df.loc[best_idx]\n",
        "    # print(\"Corresponding Metrics:\")\n",
        "    # print(best_row.to_dict())\n",
        "\n",
        "    # Plot all metrics vs. lambda\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for metric in [\"Fnorm\", \"KL\", \"MCC\", \"sensitivity\", \"specificity\", \"f1\"]:\n",
        "        plt.plot(metrics_df[\"lambda\"], metrics_df[metric], marker=\"o\", label=metric)\n",
        "\n",
        "    plt.xscale(\"log\")\n",
        "    plt.xlabel(\"Lambda\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"Metrics vs. Lambda\")\n",
        "    plt.legend()\n",
        "    plot_path = os.path.join(log_dir, \"metrics_vs_lambda.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "F1 vs sample scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "\n",
        "def plot_f1_vs_sample_scaling(\n",
        "    n=50,\n",
        "    alpha=0.92,\n",
        "    best_lambda=1.0,\n",
        "    rho=7.0,\n",
        "    max_iter=500,\n",
        "    tol=1e-4,\n",
        "    threshold=1e-2,\n",
        "    scaling_values=None,\n",
        "    log_dir=\"experiment_scale_logs\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Fix parameters (n, alpha, best_lambda, etc.) and update sample_scaling from\n",
        "    1..20 to see how F1 improves with more samples.\n",
        "    \"\"\"\n",
        "    if scaling_values is None:\n",
        "        # By default, range from 1 to 20 inclusive\n",
        "        scaling_values = np.arange(1, 21)\n",
        "\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for scale in scaling_values:\n",
        "        # Generate data with the current scale\n",
        "        B, S, N = generate_sparse_covariance(\n",
        "            n=n,\n",
        "            sample_scaling=scale,\n",
        "            alpha=alpha\n",
        "        )\n",
        "\n",
        "        # Estimate precision matrix using the best param config\n",
        "        B_est = admm_precision_matrix(S, best_lambda, rho, max_iter, tol)\n",
        "        \n",
        "        # Threshold the estimate\n",
        "        B_est_thresholded = hard_threshold(B_est, threshold)\n",
        "\n",
        "        # Compute adjacency-based F1\n",
        "        ground_truth = (B != 0).astype(int).ravel()\n",
        "        predicted = (B_est_thresholded != 0).astype(int).ravel()\n",
        "        f1_val = f1_score(ground_truth, predicted, zero_division=0)\n",
        "\n",
        "        f1_scores.append(f1_val)\n",
        "\n",
        "        print(f\"scale={scale}, #samples={N}, F1={f1_val:.3f}\")\n",
        "\n",
        "    # Convert to DataFrame for convenience\n",
        "    df = pd.DataFrame({\n",
        "        \"sample_scaling\": scaling_values,\n",
        "        \"F1\": f1_scores\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = os.path.join(log_dir, f\"f1_vs_scaling_{n}.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(scaling_values, f1_scores, marker=\"o\")\n",
        "    plt.xlabel(\"N/(d^2log(n))\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.title(f\"F1 vs. Sample Scaling (n={n}, alpha={alpha}, lambda={best_lambda})\")\n",
        "    plt.grid(True)\n",
        "    plot_path = os.path.join(log_dir, f\"f1_vs_scaling_{n}.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max degree in B 28\n",
            "ADMM Converged after 137 iterations.\n",
            "scale=1.0, #samples=4153, F1=0.364\n",
            "Max degree in B 28\n",
            "ADMM Converged after 137 iterations.\n",
            "scale=1.6896551724137931, #samples=7018, F1=0.577\n",
            "Max degree in B 28\n",
            "ADMM Converged after 136 iterations.\n",
            "scale=2.3793103448275863, #samples=9883, F1=0.747\n",
            "Max degree in B 28\n",
            "ADMM Converged after 135 iterations.\n",
            "scale=3.0689655172413794, #samples=12748, F1=0.864\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=3.7586206896551726, #samples=15612, F1=0.932\n",
            "Max degree in B 28\n",
            "ADMM Converged after 132 iterations.\n",
            "scale=4.448275862068966, #samples=18477, F1=0.961\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=5.137931034482759, #samples=21342, F1=0.982\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=5.827586206896552, #samples=24207, F1=0.993\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=6.517241379310345, #samples=27071, F1=0.996\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=7.206896551724139, #samples=29936, F1=0.997\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=7.8965517241379315, #samples=32801, F1=0.997\n",
            "Max degree in B 28\n",
            "ADMM Converged after 132 iterations.\n",
            "scale=8.586206896551724, #samples=35666, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 135 iterations.\n",
            "scale=9.275862068965518, #samples=38530, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=9.965517241379311, #samples=41395, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 135 iterations.\n",
            "scale=10.655172413793103, #samples=44260, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=11.344827586206897, #samples=47125, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=12.03448275862069, #samples=49989, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=12.724137931034484, #samples=52854, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 135 iterations.\n",
            "scale=13.413793103448278, #samples=55719, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=14.10344827586207, #samples=58584, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=14.793103448275863, #samples=61448, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=15.482758620689657, #samples=64313, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=16.17241379310345, #samples=67178, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=16.862068965517242, #samples=70043, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=17.551724137931036, #samples=72907, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=18.24137931034483, #samples=75772, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=18.931034482758623, #samples=78637, F1=0.998\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=19.620689655172416, #samples=81502, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 134 iterations.\n",
            "scale=20.310344827586206, #samples=84366, F1=0.999\n",
            "Max degree in B 28\n",
            "ADMM Converged after 133 iterations.\n",
            "scale=21.0, #samples=87231, F1=0.999\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_scaling</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.363705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.689655</td>\n",
              "      <td>0.577026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.379310</td>\n",
              "      <td>0.747210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.068966</td>\n",
              "      <td>0.864021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.758621</td>\n",
              "      <td>0.931862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.448276</td>\n",
              "      <td>0.961424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.137931</td>\n",
              "      <td>0.981781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.827586</td>\n",
              "      <td>0.992850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.517241</td>\n",
              "      <td>0.995897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7.206897</td>\n",
              "      <td>0.997432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7.896552</td>\n",
              "      <td>0.996917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>8.586207</td>\n",
              "      <td>0.998459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>9.275862</td>\n",
              "      <td>0.998458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>9.965517</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>10.655172</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>11.344828</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>12.034483</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>12.724138</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>13.413793</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>14.103448</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>14.793103</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>15.482759</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>16.172414</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>16.862069</td>\n",
              "      <td>0.997940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>17.551724</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>18.241379</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>18.931034</td>\n",
              "      <td>0.998456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>19.620690</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>20.310345</td>\n",
              "      <td>0.999486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.998971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sample_scaling        F1\n",
              "0         1.000000  0.363705\n",
              "1         1.689655  0.577026\n",
              "2         2.379310  0.747210\n",
              "3         3.068966  0.864021\n",
              "4         3.758621  0.931862\n",
              "5         4.448276  0.961424\n",
              "6         5.137931  0.981781\n",
              "7         5.827586  0.992850\n",
              "8         6.517241  0.995897\n",
              "9         7.206897  0.997432\n",
              "10        7.896552  0.996917\n",
              "11        8.586207  0.998459\n",
              "12        9.275862  0.998458\n",
              "13        9.965517  0.998971\n",
              "14       10.655172  0.998456\n",
              "15       11.344828  0.998971\n",
              "16       12.034483  0.998456\n",
              "17       12.724138  0.998456\n",
              "18       13.413793  0.998456\n",
              "19       14.103448  0.998971\n",
              "20       14.793103  0.998971\n",
              "21       15.482759  0.998456\n",
              "22       16.172414  0.998456\n",
              "23       16.862069  0.997940\n",
              "24       17.551724  0.998971\n",
              "25       18.241379  0.998456\n",
              "26       18.931034  0.998456\n",
              "27       19.620690  0.998971\n",
              "28       20.310345  0.999486\n",
              "29       21.000000  0.998971"
            ]
          },
          "execution_count": 340,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plot_f1_vs_sample_scaling(\n",
        "        n=200,\n",
        "        alpha=0.98,\n",
        "        best_lambda=0,  \n",
        "        rho=7.0,\n",
        "        max_iter=500,\n",
        "        tol=1e-4,\n",
        "        threshold=1e-2,\n",
        "        scaling_values=np.linspace(1, 21, 30), \n",
        "        log_dir=\"f1_scale_experiment\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
